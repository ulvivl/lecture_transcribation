{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "066b5b18-a610-4128-9465-23903923c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory.chat_message_histories import RedisChatMessageHistory\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.chat_history import BaseChatMessageHistory\n",
    "from langchain.schema.runnable.history import RunnableWithMessageHistory\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribations = []\n",
    "for i in range(1, 7):\n",
    "    with open(f'./data/audio{i}.json', 'r') as f:\n",
    "        transcribations.append(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribation = transcribations[0]\n",
    "\n",
    "full_text = '\\n'.join([seg['text'].strip() for seg in transcribation['segments']])\n",
    "full_text = full_text.replace('  ', ' ').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.43s/it]\n"
     ]
    }
   ],
   "source": [
    "gpu_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"IlyaGusev/saiga_mistral_7b_merged\",\n",
    "    task=\"text-generation\",\n",
    "    device=0,  # -1 for CPU\n",
    "    batch_size=2,  # adjust as needed based on GPU map and model size.\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.01, \n",
    "        \"max_length\": 1536, \n",
    "        \"exponential_decay_length_penalty\": (1280, 2.5),\n",
    "        \"top_p\": 0.9, \n",
    "        \"do_sample\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exctract intro and conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACT_INTRO_PROMPT = '''\n",
    "Тебе будет дан текст, сожержащий начало лекции. Напиши вступление для конспекта этой лекции.\n",
    "Вступление обязательно содержит 3 пункта: \n",
    "1. тема лекции\n",
    "2. какие вопросы будут рассматриваться в лекции\n",
    "3. обосновывается актуальность и значимость темы (зачем студенту изучать эту лекцию, каких ошибок студент избежит освоив эту тему).\n",
    "\n",
    "Текст:\n",
    "BEGIN_TEXT\n",
    "%s\n",
    "END_TEXT\n",
    "Ответ:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACT_CONCLUSION_PROMPT = '''\n",
    "Тебе будет дан текст, сожержащий конец лекции. Напиши заключение для конспекта этой лекции.\n",
    "В заключении должно быть два-три предложения, подводящих итог лекции, кратко описывающих пройденный материал и знания, полученные на лекции.\n",
    "\n",
    "Текст:\n",
    "BEGIN_TEXT\n",
    "%s\n",
    "END_TEXT\n",
    "Ответ:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE_INTRO_PROMPT = '''\n",
    "# Тебе будет дан текст, сожержащий начало лекции. Удали из него вступительное слово.\n",
    "# Во вступлении может обсуждаться тема лекции, какие вопросы будут рассматриваться в лекции, а также актуальность.\n",
    "# Удали из текста все предложения, относящиеся к вступлению и оставь только основную часть.\n",
    "\n",
    "# Текст:\n",
    "# BEGIN_TEXT\n",
    "# %s\n",
    "# END_TEXT\n",
    "# Ответ:\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE_CONCLUSION_PROMPT = '''\n",
    "# Тебе будет дан текст, сожержащий конец лекции. Удали из него заключение.\n",
    "# Заключение может содержать несколько предложения, подводящих итог лекции, кратко описывающих пройденный материал и знания, полученные на лекции.\n",
    "# Удали из текста все предложения, относящиеся к заключению и оставь только основную часть.\n",
    "\n",
    "# Текст:\n",
    "# BEGIN_TEXT\n",
    "# %s\n",
    "# END_TEXT\n",
    "# Ответ:\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PROMPT = '''\n",
    "{prompt}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('IlyaGusev/saiga_mistral_7b_merged')\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer=tokenizer,\n",
    "    chunk_size = 512,  # 768\n",
    "    chunk_overlap = 128,\n",
    "    add_start_index = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.create_documents([full_text])\n",
    "chunks = [item.page_content for item in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition_retrieval_system = \"\"\"Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\"\"\".strip()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", definition_retrieval_system),\n",
    "        (\"human\", BASE_PROMPT)\n",
    "    ]\n",
    ")\n",
    "\n",
    "gpu_chain = prompt | gpu_llm.bind(stop=[\" bot\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/vkarlov/venvs/langchain/lib/python3.10/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "questions = [{'prompt': cur_text} for cur_text in (EXTRACT_INTRO_PROMPT % chunks[0], EXTRACT_CONCLUSION_PROMPT % chunks[-1])]\n",
    "intro, conclusion = gpu_chain.batch(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тема лекции: Массивы в языке программирования C-Sharp\n",
      "\n",
      "Вопросы, которые будут рассматриваться в лекции:\n",
      "1. Основные понятия, связанные с массивами\n",
      "2. Применение массивов в различных задачах\n",
      "3. Операции создания, заполнения и вывода массивов на экран\n",
      "4. Разновидности циклов FOR и FOREACH\n",
      "5. Изучение английского для программистов\n",
      "\n",
      "Актуальность и значимость темы:\n",
      "1. Массивы являются одним из основных типов структур данных в программировании, которые используются для хранения и обработки больших объемов информации.\n",
      "2. Они позволяют эффективно решать задачи, связанные с обработкой данных, такие как поиск, сортировка, фильтрация и т.д.\n",
      "3. Изучение массивов в рамках языка программирования C-Sharp позволяет студентам освоить основные навыки работы с данными и применить их в решении различных задач.\n",
      "4. Развитие навыков работы с массивами помогает студентам избежать ошибок при работе с большими объемами данных и улучшить эффективность программного кода.\n",
      "5. Изучение английского языка для программистов является важным аспектом, поскольку многие документации, руководства и ресурсы по программированию написаны на английском языке.\n"
     ]
    }
   ],
   "source": [
    "print(intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В заключении лекции можно подвести, что мы рассмотрели основные характеристики массивов, изучили базовые операции с ними, а также изучили разновидности циклов FOR и FOREACH. Кроме того, мы обсудили важность изучения технического английского и его применения в программировании.\n"
     ]
    }
   ],
   "source": [
    "print(conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and get themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribation = transcribations[0]\n",
    "\n",
    "full_text = ' '.join([seg['text'].strip() for seg in transcribation['segments']])\n",
    "full_text = full_text.replace('  ', ' ').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "# Load the Spacy model\n",
    "nlp = spacy.load('ru_core_news_sm')\n",
    "\n",
    "def process(text):\n",
    "    doc = nlp(text)\n",
    "    sents = list(doc.sents)\n",
    "    vecs = np.stack([sent.vector / sent.vector_norm for sent in sents])\n",
    "\n",
    "    return sents, vecs\n",
    "\n",
    "def cluster_text(sents, vecs, threshold):\n",
    "    clusters = [[0]]\n",
    "    for i in range(1, len(sents)):\n",
    "        if np.dot(vecs[i], vecs[i-1]) < threshold:\n",
    "            clusters.append([])\n",
    "        clusters[-1].append(i)\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "def clean_text(text):\n",
    "    # Add your text cleaning process here\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_paragraphs(input_text):\n",
    "    # Initialize the clusters lengths list and final texts list\n",
    "    clusters_lens = []\n",
    "    final_texts = []\n",
    "\n",
    "    # Process the chunk\n",
    "    threshold = 0.05\n",
    "    sents, vecs = process(input_text)\n",
    "\n",
    "    # Cluster the sentences\n",
    "    clusters = cluster_text(sents, vecs, threshold)\n",
    "\n",
    "    for cluster in clusters:\n",
    "        cluster_txt = clean_text(' '.join([sents[i].text for i in cluster]))\n",
    "        cluster_len = len(cluster_txt)\n",
    "        \n",
    "        # Check if the cluster is too short\n",
    "        if cluster_len < 800:\n",
    "            continue\n",
    "        \n",
    "        # Check if the cluster is too long\n",
    "        elif cluster_len > 3000:\n",
    "            threshold = 0.1\n",
    "            sents_div, vecs_div = process(cluster_txt)\n",
    "            reclusters = cluster_text(sents_div, vecs_div, threshold)\n",
    "            \n",
    "            for subcluster in reclusters:\n",
    "                div_txt = clean_text(' '.join([sents_div[i].text for i in subcluster]))\n",
    "                div_len = len(div_txt)\n",
    "                \n",
    "                if div_len < 800 or div_len > 3000:\n",
    "                    continue\n",
    "                \n",
    "                clusters_lens.append(div_len)\n",
    "                final_texts.append(div_txt)\n",
    "                \n",
    "        else:\n",
    "            clusters_lens.append(cluster_len)\n",
    "            final_texts.append(cluster_txt)\n",
    "            \n",
    "    return final_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = split_into_paragraphs(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.73s/it]\n"
     ]
    }
   ],
   "source": [
    "gpu_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"IlyaGusev/saiga_mistral_7b_merged\",\n",
    "    task=\"text-generation\",\n",
    "    device=0,  # -1 for CPU\n",
    "    batch_size=16,  # adjust as needed based on GPU map and model size.\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.01,\n",
    "        \"max_length\": 2048,\n",
    "        # \"exponential_decay_length_penalty\": (2000, 2.5),\n",
    "        \"top_p\": 0.9, \n",
    "        \"do_sample\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "THEME_PROMPT = '''\n",
    "Тебе будет дан текст, сожержащий отрывок лекции. Придумай короткую тему для данного отрывка, отражающую его суть.\n",
    "Придуманная тема не должна быть длинной, она должна содержать меньше 8 слов.\n",
    "\n",
    "Текст:\n",
    "BEGIN_TEXT\n",
    "{text}\n",
    "END_TEXT\n",
    "Ответ:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition_retrieval_system = \"\"\"Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\"\"\".strip()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", definition_retrieval_system),\n",
    "        (\"human\", THEME_PROMPT)\n",
    "    ]\n",
    ")\n",
    "\n",
    "gpu_chain = prompt | gpu_llm.bind(stop=[\" bot\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/vkarlov/venvs/langchain/lib/python3.10/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "questions = [{'text': cur_text} for cur_text in paragraphs]\n",
    "tokenizer_kwargs={\n",
    "    'truncation':True,'max_length':1536\n",
    "}\n",
    "themes = gpu_chain.batch(questions, tokenizer_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Четные числа в массиве\n",
      "====================================================================================================\n",
      "Двигаемся далее. И давайте рассмотрим типовую задачу на поиске элементов в массиве, которые удовлетворяют некоторым условиям. Пусть у нас имеется массив на некоторое количество целых чисел, допустим, на 10 штук. Требуется найти в нем четные числа и вывести их на экран. Друзья, напомню, что число является четным, если оно делится нацело на 2. Примеры четных чисел это 0, 2, 4, 6 и так далее. Давайте для этой задачи перечислим основные этапы, сформируем ее блок-схему и реализуем эту схему на языке C-sharp. Итак, в этой задаче можно выделить следующие этапы. Первое – создать и заполнить массив на 10 цветочных чисел. Вторым этапом можно выделить проход по элементам, то есть просмотр каждого элемента. Третий этап – это проверка элемента на условия того, является ли он четным. Ну и четвертый этап, друзья, это вывести этот элемент на экран, если указанное условие выполняется.\n"
     ]
    }
   ],
   "source": [
    "i = 4\n",
    "print(themes[i])\n",
    "print('=' * 100)\n",
    "print(paragraphs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paragraphs.joblib']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(themes, 'themes.joblib')\n",
    "joblib.dump(paragraphs, 'paragraphs.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "from docx.shared import Pt\n",
    "from docx.enum.style import WD_STYLE_TYPE\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docx(introduction, main_body, conclusion, doc_name='abstract'):\n",
    "    document = Document()\n",
    "    document.add_heading('Конспект лекции', 0)\n",
    "    document.add_heading(f\"Введение\", level=1)\n",
    "\n",
    "    SENTENCE1 = 'Тема лекции'\n",
    "    SENTENCE2 = 'Вопросы, которые будут рассматриваться в лекции'\n",
    "    SENTENCE3 = 'Актуальность и значимость темы'\n",
    "\n",
    "    ind_start1 = introduction.find(SENTENCE1) \n",
    "    if ind_start1 == -1:\n",
    "        document.add_paragraph(introduction)\n",
    "        document.add_heading(f\"Основная часть\", level=1)\n",
    "        document.add_paragraph(main_body)\n",
    "\n",
    "        document.add_heading(f\"Заключение\", level=1)\n",
    "        document.add_paragraph(conclusion)\n",
    "        document.save(f'{doc_name}.docx')\n",
    "        return document\n",
    "    \n",
    "    introduction = introduction[ind_start1:]\n",
    "    document.add_heading(f\"{SENTENCE1}:\", level=3)\n",
    "    introduction = re.sub(f\"{SENTENCE1}:\", '', introduction)\n",
    "\n",
    "    ind_start2 = introduction.find(SENTENCE2)\n",
    "    if ind_start2 == -1:\n",
    "        document = Document()\n",
    "        document.add_heading('Конспект лекции', 0)\n",
    "        document.add_heading(f\"Введение\", level=1)\n",
    "        document.add_paragraph(introduction)\n",
    "        document.add_heading(f\"Основная часть\", level=1)\n",
    "        document.add_paragraph(main_body)\n",
    "\n",
    "        document.add_heading(f\"Заключение\", level=1)\n",
    "        document.add_paragraph(conclusion)\n",
    "        document.save(f'{doc_name}.docx')\n",
    "        return document\n",
    "    document.add_paragraph(introduction[:ind_start2].strip())\n",
    "\n",
    "    introduction = introduction[ind_start2:]\n",
    "    introduction = re.sub(f\"{SENTENCE2}:\", '', introduction)\n",
    "\n",
    "    document.add_heading(f\"{SENTENCE2}:\", level=3)\n",
    "    ind_start3 = introduction.find(SENTENCE3)\n",
    "    if ind_start2 == -1:\n",
    "        document = Document()\n",
    "        document.add_heading('Конспект лекции', 0)\n",
    "        document.add_heading(f\"Введение\", level=1)\n",
    "        document.add_paragraph(introduction)\n",
    "        document.add_heading(f\"Основная часть\", level=1)\n",
    "        document.add_paragraph(main_body)\n",
    "\n",
    "        document.add_heading(f\"Заключение\", level=1)\n",
    "        document.add_paragraph(conclusion)\n",
    "        document.save(f'{doc_name}.docx')\n",
    "        return document\n",
    "    \n",
    "    document.add_paragraph(introduction[: ind_start3].strip())\n",
    "\n",
    "    introduction = introduction[ind_start3:]\n",
    "    introduction = re.sub(f\"{SENTENCE3}:\", '', introduction)\n",
    "    document.add_heading(f\"{SENTENCE3}:\", level=3)\n",
    "    \n",
    "    document.add_paragraph(introduction[:].strip())\n",
    "    # document.add_page_break()\n",
    "\n",
    "    # Обработка основной части\n",
    "    themes, paragraphs = main_body\n",
    "    document.add_heading(f\"Основная часть\", level=1)\n",
    "    for theme, paragraph in zip(themes, paragraphs):    \n",
    "        document.add_heading(theme, level=3)\n",
    "        document.add_paragraph('\\t' + paragraph)\n",
    "\n",
    "    document.add_heading(f\"Заключение\", level=1)\n",
    "    document.add_paragraph('\\t' + conclusion.strip())\n",
    "\n",
    "    document.save(f'{doc_name}.docx')\n",
    "\n",
    "    return document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<docx.document.Document at 0x7fa71c642860>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_docx(intro, (themes, paragraphs), conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constuct front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_front(introduction, main_body, conclusion):\n",
    "    no_intro_format_flag = 0\n",
    "\n",
    "    # Introduction\n",
    "    intro = {\n",
    "        \"title\": f\"Введение\",\n",
    "        \"subtitles\": []\n",
    "    }\n",
    "    \n",
    "    SENTENCE1 = 'Тема лекции'\n",
    "    SENTENCE2 = 'Вопросы, которые будут рассматриваться в лекции'\n",
    "    SENTENCE3 = 'Актуальность и значимость темы'\n",
    "\n",
    "    ## тема\n",
    "    ind_start1 = introduction.find(SENTENCE1) \n",
    "    if ind_start1 == -1:\n",
    "        no_intro_format_flag = 1\n",
    "    introduction = introduction[ind_start1:]\n",
    "    introduction = re.sub(f\"{SENTENCE1}:\", '', introduction)\n",
    "    ind_start2 = introduction.find(SENTENCE2)\n",
    "    if ind_start2 == -1:\n",
    "        no_intro_format_flag = 1\n",
    "    intro['subtitles'].append(\n",
    "            {\n",
    "                \"subtitle\": f\"{SENTENCE1}:\",\n",
    "                \"texts\": [{ 'text': introduction[:ind_start2].strip()}]\n",
    "            }\n",
    "    )\n",
    "    ## вопросы\n",
    "    introduction = introduction[ind_start2:]\n",
    "    introduction = re.sub(f\"{SENTENCE2}:\", '', introduction)\n",
    "    ind_start3 = introduction.find(SENTENCE3)\n",
    "    if ind_start2 == -1:\n",
    "        no_intro_format_flag = 1\n",
    "    intro['subtitles'].append(\n",
    "            {\n",
    "                \"subtitle\": f\"{SENTENCE2}:\",\n",
    "                \"texts\": [{ 'text': introduction[: ind_start3].strip()}]\n",
    "            }\n",
    "    )\n",
    "    ## актуальность\n",
    "    introduction = introduction[ind_start3:]\n",
    "    introduction = re.sub(f\"{SENTENCE3}:\", '', introduction)\n",
    "    intro['subtitles'].append(\n",
    "            {\n",
    "                \"subtitle\": f\"{SENTENCE3}:\",\n",
    "                \"texts\": [{ 'text': introduction[:].strip()}]\n",
    "            }\n",
    "    )\n",
    "    ## cобираем вместе, если что-то не распарсилось, то сбрасываем форматирование\n",
    "    if no_intro_format_flag:\n",
    "        intro['subtitles'] = [\n",
    "            {\n",
    "                \"subtitle\": None,\n",
    "                \"texts\": [{ 'text': introduction }]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "    # Обработка основной части\n",
    "    themes, paragraphs = main_body\n",
    "    main = {\n",
    "        \"title\": f\"Основная часть\",\n",
    "        \"subtitles\": []\n",
    "    }\n",
    "    for theme, paragraph in zip(themes, paragraphs): \n",
    "        main['subtitles'].append(\n",
    "            {\n",
    "                \"subtitle\": theme,\n",
    "                \"texts\": [{ 'text': '\\t' + paragraph }]\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Обработка заключения\n",
    "    concl = {\n",
    "        \"title\": f\"Заключение\",\n",
    "        \"subtitles\": []\n",
    "    }\n",
    "    concl['subtitles'].append(\n",
    "        {\n",
    "            \"subtitle\": None,\n",
    "            \"texts\": ['\\t' + conclusion.strip()]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return [intro, main, concl]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "front_output = format_front(intro, (themes, paragraphs), conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('best_front.txt', 'w') as f:\n",
    "    print(front_output, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('best_front.json', 'w') as f:\n",
    "    json.dump(front_output, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract timecodes from whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribation = transcribations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "\n",
    "from langchain.memory.chat_message_histories import RedisChatMessageHistory\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.chat_history import BaseChatMessageHistory\n",
    "from langchain.schema.runnable.history import RunnableWithMessageHistory\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents.base import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.336"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcribation['segments'][0]['start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_texts_source = [Document(page_content=cur_segment['text'], metadata={'start': cur_segment['start']}) for cur_segment in transcribation['segments']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=' Друзья, привет!', metadata={'start': 5.336})"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_texts_source[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
    "\n",
    "# asr_texts_source = [cur_segment['text'] for cur_segment in transcribation['segments']]\n",
    "# asr_text_db = Chroma.from_texts(asr_texts_source, embedding=embedder)\n",
    "asr_text_db = Chroma.from_documents(asr_texts_source, embedding=embedder)\n",
    "asr_text_retriever = asr_text_db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начнем с ключевого термина этой лекции – массив. {}\n",
      "В данном случае это целое число. {}\n",
      "Тут некоторые значения по умолчанию нам {}\n",
      "Тут некоторые значения по умолчанию нам {}\n"
     ]
    }
   ],
   "source": [
    "timecodes = {}  # термин -- старт\n",
    "\n",
    "all_terms = ['массив', 'число', 'рекурсия', 'полет']\n",
    "for term in all_terms:\n",
    "    top_doc = asr_text_retriever.invoke(f'определение {term}')[0]\n",
    "    print(top_doc.page_content, top_doc.metadata)\n",
    "    # for i, segment in enumerate(transcribation['segments']):\n",
    "    #     if segment['text'].find(top_doc) != -1:\n",
    "    #         print()\n",
    "    #         timecodes[term] = segment['start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'массив': 66.493, 'число': 3159.633}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timecodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Тут некоторые значения по умолчанию нам'),\n",
       " Document(page_content=' который мы выделили для массива.'),\n",
       " Document(page_content='В, например, намокоз'),\n",
       " Document(page_content='Четных, а также добавим нечетные.')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term = 'полет'\n",
    "asr_text_retriever.invoke(f'определение {term}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, segment in enumerate(transcribation['segments']):\n",
    "    if segment['text'] == 'Тут некоторые значения по умолчанию нам': print(segment['text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'массив': 66.493, 'число': 3159.633}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timecodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_doc = asr_text_retriever.invoke('определение массив')[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.336"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcribation['segments'][0]['start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.493\n"
     ]
    }
   ],
   "source": [
    "timecodes = []\n",
    "for i, segment in enumerate(transcribation['segments']):\n",
    "    if segment['text'] == top_doc:\n",
    "        print(segment['start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
